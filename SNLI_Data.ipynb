{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load SNLI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "label_to_number={\"contradiction\":0, \"entailment\":1,  \"neutral\":2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snli_data_loader(filepath):\n",
    "    data_loaded=[]\n",
    "    with open(filepath) as tsvfile:\n",
    "        reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "        for i, (row) in enumerate (reader):\n",
    "            if i<1:\n",
    "                pass;\n",
    "            else:\n",
    "                sentence1=row[0].split()\n",
    "                sentence2=row[1].split()\n",
    "                label=label_to_number[row[2]]\n",
    "                data_loaded.append([sentence1, sentence2, label])\n",
    "        \n",
    "    return data_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_data_train = snli_data_loader(\"./raw_data/snli_train.tsv\")\n",
    "snli_data_val = snli_data_loader(\"./raw_data/snli_val.tsv\")\n",
    "# max_length1=max ([len(instance[0]) for instance in data])\n",
    "# max_length2=max ([len(instance[1]) for instance in data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(filepath, words_to_load=100000, PAD_IDX = 0, UNK_IDX = 1):\n",
    "\n",
    "    with open(filepath) as ft_vec:\n",
    "        loaded_embeddings_ft = np.zeros((words_to_load+2, 300))\n",
    "        np.random.seed(1)\n",
    "        loaded_embeddings_ft[UNK_IDX] = np.random.rand(300)\n",
    "        \n",
    "        token2id = {'<pad>':PAD_IDX, '<unk>':UNK_IDX}\n",
    "        id2token = {PAD_IDX:'<pad>', UNK_IDX:'<unk>'}\n",
    "        ##bypass thhe first line \n",
    "        next(ft_vec)\n",
    "        for i, line in enumerate(ft_vec):\n",
    "            if i == words_to_load: \n",
    "                break\n",
    "            else:\n",
    "                row = line.split()\n",
    "                loaded_embeddings_ft[i+2] = np.asarray(row[1:])\n",
    "                token2id[row[0]] = i+2\n",
    "                id2token[i+2] = row[0]\n",
    "\n",
    "    return loaded_embeddings_ft, token2id, id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_embeddings_ft, token2id, id2token=build_vocab(\"./fasttext_vector/wiki-news-300d-1M.vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# index data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2index_dataset(snli_data, token2id, PAD_IDX = 0, UNK_IDX = 1):\n",
    "    processed_snli_data = []\n",
    "    \n",
    "    for instance in snli_data:\n",
    "        sentence1 = [token2id[token] if token in token2id else UNK_IDX for token in instance[0]]\n",
    "        sentence2 = [token2id[token] if token in token2id else UNK_IDX for token in instance[1]]\n",
    "        processed_snli_data.append([sentence1, sentence2, instance[2]])\n",
    "    \n",
    "    return processed_snli_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_snli_data_train=token2index_dataset(snli_data_train, token2id)\n",
    "processed_snli_data_val=token2index_dataset(snli_data_val, token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "pkl.dump(processed_snli_data_train, open(\"./data/processed_snli_data_train.p\", \"wb\"))\n",
    "pkl.dump(processed_snli_data_val, open(\"./data/processed_snli_data_val.p\", \"wb\"))\n",
    "pkl.dump(loaded_embeddings_ft, open(\"./data/loaded_embeddings_ft.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "Genres = [\"fiction\", \"telephone\", \"slate\", \"government\", \"travel\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def mnli_data_loader(filepath):\n",
    "    data_loaded={\n",
    "        \"fiction\": [],\n",
    "        \"telephone\": [],\n",
    "        \"slate\": [],\n",
    "        \"government\":[],\n",
    "        \"travel\":[]\n",
    "    }\n",
    "    with open(filepath) as tsvfile:\n",
    "        reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "        for i, (row) in enumerate (reader):\n",
    "            if i<1:\n",
    "                pass;\n",
    "            else:\n",
    "                sentence1=row[0].split()\n",
    "                sentence2=row[1].split()\n",
    "                label=label_to_number[row[2]]\n",
    "                genre = row[3]\n",
    "                data_loaded[genre].append([sentence1, sentence2, label])\n",
    "    return data_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_data_val= mnli_data_loader(\"./raw_data/mnli_val.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2index_dataset_mnli(mnli_data, token2id, PAD_IDX = 0, UNK_IDX = 1):\n",
    "    \n",
    "    processed_mnli_data = {\n",
    "        \"fiction\": [],\n",
    "        \"telephone\": [],\n",
    "        \"slate\": [],\n",
    "        \"government\":[],\n",
    "        \"travel\":[]\n",
    "    }\n",
    "    for genre in Genres:\n",
    "        for instance in mnli_data[genre]:\n",
    "            sentence1 = [token2id[token] if token in token2id else UNK_IDX for token in instance[0]]\n",
    "            sentence2 = [token2id[token] if token in token2id else UNK_IDX for token in instance[1]]\n",
    "            processed_mnli_data[genre].append([sentence1, sentence2, instance[2]])\n",
    "    \n",
    "    return processed_mnli_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_mnli_data_val=token2index_dataset_mnli(mnli_data_val, token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "pkl.dump(processed_mnli_data_val, open(\"./data/processed_mnli_data_val.p\", \"wb\"))\n",
    "# pkl.dump(loaded_embeddings_ft, open(\"./data/loaded_embeddings_ft.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
